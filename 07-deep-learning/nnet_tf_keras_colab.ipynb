{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nnet-class-2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SfayRXTLqi0f"
      },
      "source": [
        "# Neural Networks with scikit-learn\n",
        "\n",
        "Neural networks are amongst the most complex and flexible machine/deep learning models, due to this, their capability to tackle more complex problems is huge. However, due to this flexibility, they are also easier to overfit, which is why it's the data scientists job to find the correct hyper-parameters for these models.\n",
        "\n",
        "In order to do this, we'll use scikit learn in the same way we always have. In this case, however, neural networks have a lot more than 1 parameter, in case of the multi-layer perceptron models in scikit-learn the parameters are:\n",
        "\n",
        "* hidden_layer_sizes\n",
        "* activation\n",
        "* solver\n",
        "* alpha\n",
        "* batch_size\n",
        "* learning_rate\n",
        "* learning_rate_init\n",
        "* power_t\n",
        "* max_iter\n",
        "* shuffle\n",
        "* random_state\n",
        "* tol\n",
        "* verbose\n",
        "* warm_start\n",
        "* momentum\n",
        "* nesterovs_momentum\n",
        "* early_stopping\n",
        "* validation_fraction\n",
        "* beta_1\n",
        "* beta_2\n",
        "* epsilon\n",
        "* n_iter_no_change\n",
        "\n",
        "You can read more about each one [here](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html).\n",
        "\n",
        "## The data (MNIST)\n",
        "\n",
        "Let's load the digit data from MNIST."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uFTzN8HMZ9mX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4evrqrAhYOhV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_train_id = '1W-PGHpcl6FFV_V1eaPNsyJms9laSdnmt'\n",
        "file_train = drive.CreateFile({'id': file_train_id})\n",
        "file_train.GetContentFile(\"train.csv\")\n",
        "\n",
        "file_test_id = '16uEssOvapZsNvscKcb9KdyRhosPEBukq'\n",
        "file_test = drive.CreateFile({'id': file_test_id})\n",
        "file_test.GetContentFile(\"test.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "bl4QgjATp2Nk",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"train.csv\")\n",
        "test_data = pd.read_csv(\"test.csv\") # Only for Kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OsQLymscYFJi",
        "colab_type": "text"
      },
      "source": [
        "We visualize the already familiar data with the modified label type to category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e87ktjHYFJj",
        "colab_type": "code",
        "outputId": "2f446920-014b-4d07-bc6a-76aad2fbad82",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "data.label = data.label.astype(\"category\")\n",
        "\n",
        "data.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel0</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>pixel11</th>\n",
              "      <th>pixel12</th>\n",
              "      <th>pixel13</th>\n",
              "      <th>pixel14</th>\n",
              "      <th>pixel15</th>\n",
              "      <th>pixel16</th>\n",
              "      <th>pixel17</th>\n",
              "      <th>pixel18</th>\n",
              "      <th>pixel19</th>\n",
              "      <th>pixel20</th>\n",
              "      <th>pixel21</th>\n",
              "      <th>pixel22</th>\n",
              "      <th>pixel23</th>\n",
              "      <th>pixel24</th>\n",
              "      <th>pixel25</th>\n",
              "      <th>pixel26</th>\n",
              "      <th>pixel27</th>\n",
              "      <th>pixel28</th>\n",
              "      <th>pixel29</th>\n",
              "      <th>pixel30</th>\n",
              "      <th>pixel31</th>\n",
              "      <th>pixel32</th>\n",
              "      <th>pixel33</th>\n",
              "      <th>pixel34</th>\n",
              "      <th>pixel35</th>\n",
              "      <th>pixel36</th>\n",
              "      <th>pixel37</th>\n",
              "      <th>pixel38</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel744</th>\n",
              "      <th>pixel745</th>\n",
              "      <th>pixel746</th>\n",
              "      <th>pixel747</th>\n",
              "      <th>pixel748</th>\n",
              "      <th>pixel749</th>\n",
              "      <th>pixel750</th>\n",
              "      <th>pixel751</th>\n",
              "      <th>pixel752</th>\n",
              "      <th>pixel753</th>\n",
              "      <th>pixel754</th>\n",
              "      <th>pixel755</th>\n",
              "      <th>pixel756</th>\n",
              "      <th>pixel757</th>\n",
              "      <th>pixel758</th>\n",
              "      <th>pixel759</th>\n",
              "      <th>pixel760</th>\n",
              "      <th>pixel761</th>\n",
              "      <th>pixel762</th>\n",
              "      <th>pixel763</th>\n",
              "      <th>pixel764</th>\n",
              "      <th>pixel765</th>\n",
              "      <th>pixel766</th>\n",
              "      <th>pixel767</th>\n",
              "      <th>pixel768</th>\n",
              "      <th>pixel769</th>\n",
              "      <th>pixel770</th>\n",
              "      <th>pixel771</th>\n",
              "      <th>pixel772</th>\n",
              "      <th>pixel773</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "  label  pixel0  pixel1  pixel2  ...  pixel780  pixel781  pixel782  pixel783\n",
              "0     1       0       0       0  ...         0         0         0         0\n",
              "1     0       0       0       0  ...         0         0         0         0\n",
              "2     1       0       0       0  ...         0         0         0         0\n",
              "3     4       0       0       0  ...         0         0         0         0\n",
              "4     0       0       0       0  ...         0         0         0         0\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uEcBoSaYFJm",
        "colab_type": "text"
      },
      "source": [
        "We split the data set into a test and training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HwpsgXocYFJn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(data.drop(\"label\", axis = 1), data.label, test_size = 0.3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ibMAJqWyYFJp",
        "colab_type": "text"
      },
      "source": [
        "## The network\n",
        "\n",
        "This classifier is a dense neural network, which can be visualized as:\n",
        "\n",
        "![By Glosser.ca - Own work, Derivative of File:Artificial neural network.svg, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=24913461](https://github.com/ClementFelipe/ai-class-lab/blob/master/07-deep-learning/resources/nnet.png?raw=1)\n",
        "\n",
        "This network architecture is the **MLPClassifier** in scikit-learn, first we will try a network with one hidden with default parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpTVTKvFYFJp",
        "colab_type": "code",
        "outputId": "2d62bf63-2649-43e5-9e4b-6e80943524b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "net = MLPClassifier(verbose = True)\n",
        "net.fit(x_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 8.69544639\n",
            "Iteration 2, loss = 1.45531198\n",
            "Iteration 3, loss = 0.74634259\n",
            "Iteration 4, loss = 0.48005803\n",
            "Iteration 5, loss = 0.34788734\n",
            "Iteration 6, loss = 0.27587872\n",
            "Iteration 7, loss = 0.21212238\n",
            "Iteration 8, loss = 0.18024399\n",
            "Iteration 9, loss = 0.14741928\n",
            "Iteration 10, loss = 0.13244188\n",
            "Iteration 11, loss = 0.11657773\n",
            "Iteration 12, loss = 0.10906752\n",
            "Iteration 13, loss = 0.10349047\n",
            "Iteration 14, loss = 0.09122203\n",
            "Iteration 15, loss = 0.07936778\n",
            "Iteration 16, loss = 0.07627667\n",
            "Iteration 17, loss = 0.07693813\n",
            "Iteration 18, loss = 0.08383788\n",
            "Iteration 19, loss = 0.08001242\n",
            "Iteration 20, loss = 0.07839557\n",
            "Iteration 21, loss = 0.06583349\n",
            "Iteration 22, loss = 0.09413323\n",
            "Iteration 23, loss = 0.09862921\n",
            "Iteration 24, loss = 0.09091375\n",
            "Iteration 25, loss = 0.07865257\n",
            "Iteration 26, loss = 0.07017265\n",
            "Iteration 27, loss = 0.05955003\n",
            "Iteration 28, loss = 0.07659591\n",
            "Iteration 29, loss = 0.07059128\n",
            "Iteration 30, loss = 0.06556007\n",
            "Iteration 31, loss = 0.07426077\n",
            "Iteration 32, loss = 0.06726139\n",
            "Iteration 33, loss = 0.06844216\n",
            "Iteration 34, loss = 0.05927444\n",
            "Iteration 35, loss = 0.05119832\n",
            "Iteration 36, loss = 0.05077032\n",
            "Iteration 37, loss = 0.07214614\n",
            "Iteration 38, loss = 0.08069476\n",
            "Iteration 39, loss = 0.06636168\n",
            "Iteration 40, loss = 0.05365673\n",
            "Iteration 41, loss = 0.06366938\n",
            "Iteration 42, loss = 0.05371240\n",
            "Iteration 43, loss = 0.05142404\n",
            "Iteration 44, loss = 0.04745795\n",
            "Iteration 45, loss = 0.05090489\n",
            "Iteration 46, loss = 0.06514961\n",
            "Iteration 47, loss = 0.04465414\n",
            "Iteration 48, loss = 0.04071883\n",
            "Iteration 49, loss = 0.03420968\n",
            "Iteration 50, loss = 0.03562134\n",
            "Iteration 51, loss = 0.04066367\n",
            "Iteration 52, loss = 0.05234354\n",
            "Iteration 53, loss = 0.05429858\n",
            "Iteration 54, loss = 0.05824946\n",
            "Iteration 55, loss = 0.04849014\n",
            "Iteration 56, loss = 0.06121649\n",
            "Iteration 57, loss = 0.04492501\n",
            "Iteration 58, loss = 0.03650786\n",
            "Iteration 59, loss = 0.03799810\n",
            "Iteration 60, loss = 0.05107061\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.0001, batch_size='auto', beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(100,), learning_rate='constant',\n",
              "              learning_rate_init=0.001, max_iter=200, momentum=0.9,\n",
              "              n_iter_no_change=10, nesterovs_momentum=True, power_t=0.5,\n",
              "              random_state=None, shuffle=True, solver='adam', tol=0.0001,\n",
              "              validation_fraction=0.1, verbose=True, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tn8e-8jOYFJs",
        "colab_type": "text"
      },
      "source": [
        "Notice the default parameters above after the description of the epochs (iterations).\n",
        "\n",
        "Now we can visualize it's accuracy in the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDz77g59YFJs",
        "colab_type": "code",
        "outputId": "2cd5520f-c5d4-4539-ebe9-6a00db3fefe6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "net.score(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9505555555555556"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRRZKV_UYFJv",
        "colab_type": "text"
      },
      "source": [
        "Out of the box, this model will give us an accuracy of 95-96%. Let's explore some basic parameters and what they mean.\n",
        "\n",
        "### Learning rate (learning_rate_init)\n",
        "\n",
        "This parameter controls the size of the \"steps\" a network takes when performing gradient descent, if this is too low, the network will take very long to converge, if it's too high, it might diverge instead, let's see some examples.\n",
        "\n",
        "Also below is a visualization of how the learning rates behave."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HV3_xKduYFJw",
        "colab_type": "code",
        "outputId": "1f91960a-2513-4524-fc05-4a3225eb2a25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 491
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "learning_rates = [0.0001, 0.001, 0.01, 0.1]\n",
        "scores = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"-----------Starting training with lr {lr}\")\n",
        "    model = MLPClassifier(learning_rate_init = lr, verbose = False, max_iter=25)\n",
        "    model.fit(x_train, y_train)\n",
        "    score = model.score(x_test, y_test)\n",
        "\n",
        "    plt.plot(model.loss_curve_, label=lr)\n",
        "\n",
        "    scores.append(score)\n",
        "    print(f\"-----------Finished lr {lr} with score {score}\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------Starting training with lr 0.0001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (25) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------Finished lr 0.0001 with score 0.9188095238095239\n",
            "-----------Starting training with lr 0.001\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (25) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------Finished lr 0.001 with score 0.9454761904761905\n",
            "-----------Starting training with lr 0.01\n",
            "-----------Finished lr 0.01 with score 0.8374603174603175\n",
            "-----------Starting training with lr 0.1\n",
            "-----------Finished lr 0.1 with score 0.10095238095238095\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\noIuIJIQCXUQkIRToIiIJoUAXEUmI/w9LgHOMSkUvQwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It4wBg0jYFJy",
        "colab_type": "text"
      },
      "source": [
        "We can see that low learning rates will take very long to converge and as such don't achieve good accuracy given the same number of epochs. Now we visualize the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3K4r9gKYFJz",
        "colab_type": "code",
        "outputId": "5cfa230e-f4f7-4de3-90fa-8354b07a3f85",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.plot(learning_rates, scores)\n",
        "plt.scatter(learning_rates, scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7ffa35e01780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXKtoCor9AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4foV3ycLnrLB",
        "colab_type": "text"
      },
      "source": [
        "We can see that as the learning rate increases too much, accuracy immediately drops due to the model diverging, whereas when the learning rate is low, it has lower accuracy than the higher (but not too high) learning rates after it.\n",
        "\n",
        "Now let's keep this learning rate and see the effect of the number of epochs.\n",
        "\n",
        "### Epochs (max_iter)\n",
        "\n",
        "This parameter controls how many times tha dataset will perform backpropagation, this means how many times the data will pass through (feed-forward) the data and adjust it's weights (back-propagation)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8Rn85uAnnZO",
        "colab_type": "code",
        "outputId": "5e3e39ba-2aa4-41d7-a261-25111b8c76b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        }
      },
      "source": [
        "n_iters = [2, 4, 8, 16, 32, 64, 128]\n",
        "scores = []\n",
        "\n",
        "for n in n_iters:\n",
        "    print(f\"-----------Starting training with n_iter {n}\")\n",
        "    model = MLPClassifier(verbose = False, max_iter=n, n_iter_no_change = int(n/2), tol=1e-8)\n",
        "    model.fit(x_train, y_train)\n",
        "    score = model.score(x_test, y_test)\n",
        "    scores.append(score)\n",
        "    print(f\"-----------Finished n_iter {n} with score {score}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------Starting training with n_iter 2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------Finished n_iter 2 with score 0.8969047619047619\n",
            "-----------Starting training with n_iter 4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (4) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------Finished n_iter 4 with score 0.9128571428571428\n",
            "-----------Starting training with n_iter 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (8) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------Finished n_iter 8 with score 0.9205555555555556\n",
            "-----------Starting training with n_iter 16\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (16) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------Finished n_iter 16 with score 0.9307142857142857\n",
            "-----------Starting training with n_iter 32\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (32) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------Finished n_iter 32 with score 0.948015873015873\n",
            "-----------Starting training with n_iter 64\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (64) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------Finished n_iter 64 with score 0.9566666666666667\n",
            "-----------Starting training with n_iter 128\n",
            "-----------Finished n_iter 128 with score 0.9626190476190476\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (128) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF2bTAy1rg4O",
        "colab_type": "text"
      },
      "source": [
        "We can see that as the number of epochs increases, our accuracy does too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u7LL5iDbre1q",
        "colab_type": "code",
        "outputId": "6d787386-81bb-4371-e1ff-4bc4b6395a7f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "plt.plot(n_iters, scores)\n",
        "plt.scatter(n_iters, scores)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7f7de1f2f160>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0IuIRJyCXkQk4hT0IiIR9/8BxYAdhrGUf9sAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jnoPPa9xYFJ2",
        "colab_type": "text"
      },
      "source": [
        "### Layers (hidden_layer_sizes)\n",
        "\n",
        "Now let's try modifying the layers, we will try 1 hidden layer with 100 neurons (default), 2 hidden layers with 20 and 10 neurons, 3 hidden layers with 30, 20 and 10 neurons and 4 hidden layers with 100, 30, 20, 10 neurons. We will use a learning rate of 0.001 (default)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rN40KzZcYFJ3",
        "colab_type": "code",
        "outputId": "b2df43ad-b853-4950-e234-c73e9802a950",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        }
      },
      "source": [
        "layers = [(100), (100, 10), (100, 50, 10), (100, 50, 20, 10)]\n",
        "scores = []\n",
        "\n",
        "for lyr in layers:\n",
        "    print(f\"-----------Starting training with lr {lyr}\")\n",
        "    model = MLPClassifier(hidden_layer_sizes = lyr, verbose = False, max_iter=50, n_iter_no_change = 40, tol = 1e-8)\n",
        "    model.fit(x_train, y_train)\n",
        "    score = model.score(x_test, y_test)\n",
        "    scores.append(score)\n",
        "    print(f\"-----------Finished lr {lyr} with score {score}\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----------Starting training with lr 100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------Finished lr 100 with score 0.9511111111111111\n",
            "-----------Starting training with lr (100, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------Finished lr (100, 10) with score 0.9519047619047619\n",
            "-----------Starting training with lr (100, 50, 10)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "-----------Finished lr (100, 50, 10) with score 0.9545238095238096\n",
            "-----------Starting training with lr (100, 50, 20, 10)\n",
            "-----------Finished lr (100, 50, 20, 10) with score 0.9611111111111111\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (50) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNXjV-mXYFJ5",
        "colab_type": "text"
      },
      "source": [
        "We can see that all results are similar, let's try something a bit deeper. (takes long to execute)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJEm53oSYFJ6",
        "colab_type": "code",
        "outputId": "77987831-f8a0-4daf-ab2b-092776ea6506",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "model = MLPClassifier(hidden_layer_sizes = (2500, 2000, 1500, 1000, 500), verbose = True, max_iter = 10, n_iter_no_change = 10, tol = 0.0000005)\n",
        "model.fit(x_train, y_train)\n",
        "model.score(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = inf\n",
            "Iteration 2, loss = 0.14055122\n",
            "Iteration 3, loss = 0.09505866\n",
            "Iteration 4, loss = 0.06967283\n",
            "Iteration 5, loss = 0.05815726\n",
            "Iteration 6, loss = 0.05251947\n",
            "Iteration 7, loss = 0.04786868\n",
            "Iteration 8, loss = 0.04045275\n",
            "Iteration 9, loss = 0.03728171\n",
            "Iteration 10, loss = 0.03630677\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9671428571428572"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqYS3eSzYFJ8",
        "colab_type": "text"
      },
      "source": [
        "We can see that the accuracy is immediately better than the previous models, even with a low epoch count. But how do we arrive at this model? One way to do it (if you have sufficiently powerful hardware) is to use cross validation to search combinations of parameters.\n",
        "\n",
        "In order to look for the right params, and have it not be so problematic, let's use an already defined scikit-learn function called **GridSearchCV**. This function will help us by making all the combinations of the parameters we give it and using them in the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnrarVr7YFJ-",
        "colab_type": "code",
        "outputId": "9b9df3a7-9c3b-4da5-9b4c-2d95673bd348",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        }
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "model = MLPClassifier(max_iter=10, tol=1e-8, early_stopping=True, batch_size=200)\n",
        "\n",
        "parameters = {\n",
        "    \"hidden_layer_sizes\": [(100)],\n",
        "    \"activation\": [\"tanh\", \"relu\"],\n",
        "    \"solver\": [\"lbfgs\", \"sgd\", \"adam\"],\n",
        "    \"alpha\": [0.0001, 0.00001],\n",
        "    \"learning_rate\": [\"invscaling\", \"adaptive\"],\n",
        "    \"momentum\": [0.9, 0.95]\n",
        "}\n",
        "\n",
        "search = GridSearchCV(model, parameters, cv = 5, n_jobs=-1, verbose=2)\n",
        "search.fit(data.drop(\"label\", axis = 1), data.label)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 5 folds for each of 48 candidates, totalling 240 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "/usr/local/lib/python3.6/dist-packages/joblib/externals/loky/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
            "  \"timeout or by a memory leak.\", UserWarning\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:  4.6min\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed: 18.3min\n",
            "[Parallel(n_jobs=-1)]: Done 240 out of 240 | elapsed: 26.3min finished\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/neural_network/multilayer_perceptron.py:566: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (10) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
              "             estimator=MLPClassifier(activation='relu', alpha=0.0001,\n",
              "                                     batch_size=200, beta_1=0.9, beta_2=0.999,\n",
              "                                     early_stopping=True, epsilon=1e-08,\n",
              "                                     hidden_layer_sizes=(100,),\n",
              "                                     learning_rate='constant',\n",
              "                                     learning_rate_init=0.001, max_iter=10,\n",
              "                                     momentum=0.9, n_iter_no_change=10,\n",
              "                                     nesterovs_momentum=True, power_t=0.5,\n",
              "                                     random_state=No...\n",
              "                                     solver='adam', tol=1e-08,\n",
              "                                     validation_fraction=0.1, verbose=False,\n",
              "                                     warm_start=False),\n",
              "             iid='warn', n_jobs=-1,\n",
              "             param_grid={'activation': ['tanh', 'relu'],\n",
              "                         'alpha': [0.0001, 1e-05], 'hidden_layer_sizes': [100],\n",
              "                         'learning_rate': ['invscaling', 'adaptive'],\n",
              "                         'momentum': [0.9, 0.95],\n",
              "                         'solver': ['lbfgs', 'sgd', 'adam']},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UhlIB427Tss",
        "colab_type": "code",
        "outputId": "520f30c0-f046-4f45-a1e5-00866b4c74d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "estimator=MLPClassifier(\n",
        "    activation='relu', alpha=0.0001,\n",
        "    batch_size=200, beta_1=0.9, beta_2=0.999,\n",
        "    early_stopping=True, epsilon=1e-08,\n",
        "    hidden_layer_sizes=(100,),\n",
        "    learning_rate='constant',\n",
        "    learning_rate_init=0.001, max_iter=100,\n",
        "    momentum=0.9, n_iter_no_change=10,\n",
        "    nesterovs_momentum=True, power_t=0.5,\n",
        "    solver='adam', tol=1e-08,\n",
        "    validation_fraction=0.1, verbose=False,\n",
        "    warm_start=False\n",
        ").fit(x_train, y_train)\n",
        "\n",
        "estimator.score(x_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9535714285714286"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feIt-_cUDCNK",
        "colab_type": "text"
      },
      "source": [
        "## Tensorflow + Keras\n",
        "\n",
        "Tensorflow is the famous machine learning framework from Google, with it we can perform any machine learning algorithm. It works by using a programming style combining a special mathematical construct called a Tensor (**similar** in a certain way to a multidimensional array) and graph processing to achieve desired results. Meanwhile, Keras is a specification for building Neural Networks, it happens to be implemented in many machine learning frameworks, including Tensorflow. Here we will use Keras to build a convolutional neural network to predict the digits from MNIST.\n",
        "\n",
        "Here we import Tensorflow 2 using the python magic (this magic only exists in Colab)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BkNI6igODBUL",
        "colab_type": "code",
        "outputId": "7525538e-d2dc-4495-f754-c9a666be515e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X588KnWMIEKT",
        "colab_type": "text"
      },
      "source": [
        "Now we can load the MNIST dataset, which come along with a variety of other datasets in keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c8PYdXMSIDrW",
        "colab_type": "code",
        "outputId": "05c33892-205b-4c33-f1c0-18330ca8bd1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 857
        }
      },
      "source": [
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "x_train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]],\n",
              "\n",
              "       [[0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        ...,\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.],\n",
              "        [0., 0., 0., ..., 0., 0., 0.]]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6XPEmiucGVCc",
        "colab_type": "text"
      },
      "source": [
        "The previous dataset, as can be seen is a multidimensional array representing the matrix of values for the digits. Now we will create a regular dense model using keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz4Gw-D6GUCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.2),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5T2_iaLG58R",
        "colab_type": "text"
      },
      "source": [
        "The above lines create the model in a simple high level API implemented in Tensorflow for Keras. Now we will fit the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WpzcmpzG5PY",
        "colab_type": "code",
        "outputId": "7dd7f604-5594-4d90-8023-49b2473b32e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 6s 105us/sample - loss: 0.2935 - accuracy: 0.9153\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 6s 97us/sample - loss: 0.1444 - accuracy: 0.9568\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.1082 - accuracy: 0.9665\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 5s 87us/sample - loss: 0.0886 - accuracy: 0.9726\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 5s 88us/sample - loss: 0.0740 - accuracy: 0.9767\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f928bc23be0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxWs_R1KKDLs",
        "colab_type": "text"
      },
      "source": [
        "This output show us each epoch along with it's training error and loss, we will now obtain the validation set error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7n77C5eJklJ",
        "colab_type": "code",
        "outputId": "8b2574d0-37a6-4177-ded4-99d7ee907673",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(x_test,  y_test, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/1 - 1s - loss: 0.0397 - accuracy: 0.9766\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07721703117198776, 0.9766]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZP3zpNMJlTW",
        "colab_type": "text"
      },
      "source": [
        "As we can see, we get an accuracy of 97% in this training with just 5 epochs. We have to take into account that this dataset is different from the previous one, in the sense that it has a lot more data in the training set, this, in part, is responsible for the out of the box increase in the accuracy. We will now try a more complex convolutional neural network.\n",
        "\n",
        "We will first make some necessary transformations to the data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_7ck8itSRK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.reshape(60000,28,28,1)\n",
        "x_test = x_test.reshape(10000,28,28,1)\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UrAIv2HKcQ4",
        "colab_type": "code",
        "outputId": "037f57fd-2658-49a5-9a98-a3026fe7c18b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        }
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(28,28,1)),\n",
        "  tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "  tf.keras.layers.Dropout(0.25),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dropout(0.5),\n",
        "  tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 11s 176us/sample - loss: 0.1805 - accuracy: 0.9448\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0760 - accuracy: 0.9770\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 7s 121us/sample - loss: 0.0576 - accuracy: 0.9825\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0485 - accuracy: 0.9851\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 0.0404 - accuracy: 0.9878\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f7d4255aac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3pA9iqyWHkI",
        "colab_type": "text"
      },
      "source": [
        "Now we can find out the validation error!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CPiFAuRQVurM",
        "colab_type": "code",
        "outputId": "cbd30048-6f50-41c1-d436-2394599c51af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "model.evaluate(x_test, y_test, verbose=2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/1 - 1s - loss: 0.0137 - accuracy: 0.9923\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.027371744615323407, 0.9923]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    }
  ]
}